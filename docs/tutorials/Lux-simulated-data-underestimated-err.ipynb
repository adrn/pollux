{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Lux*: Simulated Data and Underestimated Uncertainties\n",
    "\n",
    "In this tutorial, we will build on our [previous demonstration](Lux-linear-simulated-data.html) of [_Lux_](https://arxiv.org/abs/2502.01745) using simulated data to consider a case in which we are given data and uncertainties, but we believe the uncertainties are systematically underestimated for certain pixels. This issue sometimes appears in modeling stellar spectra, when telluric features, sky lines, or other issues are not fully accounted for in the uncertainties. We will demonstrate how to incorporate a (vector) parameter to handle this by adding an additional variance term to the likelihood, set by this parameter.\n",
    "\n",
    "As usual, we will start with some standard imports and set up the simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "\n",
    "import pollux as plx\n",
    "from pollux.models.transforms import LinearTransform\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating simulated data\n",
    "\n",
    "We will generate data for 2048 stars, with a latent dimensionality of 8, 2 labels, and 128 pixels in the spectra. We will follow the same prescription as in the [previous tutorial](Lux-linear-simulated-data.html) to generate the simulated labels and spectra. After generating the data, we will then add in a systematic error (as a function of pixel number) that is not accounted for in the reported uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import make_simulated_linear_data\n",
    "\n",
    "n_stars = 2048  # number of simulated stars to generate in the train and test sets\n",
    "n_latents = 8  # size of the latent vector per star\n",
    "n_labels = 2  # number of labels to generate per star\n",
    "n_flux = 128  # number of spectral flux pixels per star\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "A = np.zeros((n_labels, n_latents))\n",
    "A[0, 0] = 1.0\n",
    "A[1, 1] = 1.0\n",
    "\n",
    "B = rng.normal(scale=0.1, size=(n_flux, n_latents))\n",
    "B[:, 0] = B[:, 0] + 4 * np.exp(-0.5 * (np.arange(n_flux) - n_flux / 2) ** 2 / 5**2)\n",
    "B[:, 1] = B[:, 1] + 2 * np.exp(-0.5 * (np.arange(n_flux) - n_flux / 4) ** 2 / 3**2)\n",
    "\n",
    "data, truth = make_simulated_linear_data(\n",
    "    n_stars=n_stars,\n",
    "    n_latents=n_latents,\n",
    "    n_flux=n_flux,\n",
    "    n_labels=n_labels,\n",
    "    A=A,\n",
    "    B=B,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "# Now we add a periodic systematic error to the flux:\n",
    "true_systematic_err = 2.0 * (np.cos(2 * np.pi * np.arange(n_flux) / (n_flux / 4))) ** 2\n",
    "data[\"flux\"] = rng.normal(data[\"flux\"], scale=true_systematic_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The systematic error we add inflates the uncertainties significantly in a periodic pattern with pixel number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(true_systematic_err)\n",
    "plt.ylabel(\"Systematic error\")\n",
    "plt.xlabel(\"Spectral pixel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With simulated data in hand, we now proceed to run the _Lux_ model on this data.\n",
    "\n",
    "As with the previous tutorial, we will package this data (to prepare for using it in  {py:class}`~pollux.models.LuxModel`) by defining a {py:class}`~pollux.data.PolluxData` instance with the data. We use the standard shift-and-scale normalization for the spectral flux data and labels (as shown in the previous tutorial):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = plx.data.PolluxData(\n",
    "    flux=plx.data.OutputData(\n",
    "        data[\"flux\"],\n",
    "        err=data[\"flux_err\"],\n",
    "        preprocessor=plx.data.ShiftScalePreprocessor.from_data(data[\"flux\"]),\n",
    "    ),\n",
    "    label=plx.data.OutputData(\n",
    "        data[\"label\"],\n",
    "        err=data[\"label_err\"],\n",
    "        preprocessor=plx.data.ShiftScalePreprocessor.from_data(data[\"label\"]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "preprocessed_data = all_data.preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will again use the _Lux_ model in a \"supervised\" or \"train and apply\" mode, in which we will train the model on a subset of the data and then apply it to the remaining data. We will use the first 1024 stars for training and the remaining 1024 stars for testing (since they are not ordered in any way):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocessed_data[: n_stars // 2]\n",
    "test_data = preprocessed_data[n_stars // 2 :]\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the _Lux_ model\n",
    "\n",
    "In our first demonstration, we will use the same _Lux_ model as in the previous tutorial (i.e. without adding any additional parameters to learn the systematic error). We will then show that the model performs worse than a model that accounts for the (unknown) systematic error by simultaneously learning this vector.\n",
    "\n",
    "### Model 1: _Lux_ without systematic error (same as in the previous tutorial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = plx.LuxModel(latent_size=8)\n",
    "model1.register_output(\"label\", transform=LinearTransform(output_size=n_labels))\n",
    "model1.register_output(\"flux\", transform=LinearTransform(output_size=n_flux))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params1, svi_results1 = model1.optimize(\n",
    "    train_data,\n",
    "    rng_key=jax.random.PRNGKey(112358),\n",
    "    optimizer=numpyro.optim.Adam(1e-3),\n",
    "    num_steps=32768,\n",
    "    svi_run_kwargs={\"progress_bar\": False},\n",
    ")\n",
    "svi_results1.losses.block_until_ready()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the loss trajectory for the last 2000 steps to see if (visually) the optimization has converged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(svi_results1.losses[-2000:])\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function evolution looks approximately stable, so we will assume that the MAP optimization has converged. We can now evaluate the model on the test data and compare the results to the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params1 = {\n",
    "    \"label\": {\"A\": opt_params1[\"label\"][\"A\"]},\n",
    "    \"flux\": {\"A\": opt_params1[\"flux\"][\"A\"]},\n",
    "}\n",
    "\n",
    "test_opt_params1, test_svi_results1 = model1.optimize(\n",
    "    test_data,\n",
    "    rng_key=jax.random.PRNGKey(12345),\n",
    "    optimizer=numpyro.optim.Adam(1e-3),\n",
    "    num_steps=32_768,\n",
    "    fixed_params=fixed_params1,\n",
    "    svi_run_kwargs={\"progress_bar\": False},\n",
    ")\n",
    "test_svi_results1.losses.block_until_ready()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_values1 = model1.predict_outputs(\n",
    "    test_opt_params1[\"latents\"], fixed_params1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_style = {\"ls\": \"none\", \"ms\": 2.0, \"alpha\": 0.5, \"marker\": \"o\", \"color\": \"k\"}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4), layout=\"constrained\")\n",
    "for i in range(predict_test_values1[\"label\"].shape[1]):\n",
    "    axes[i].plot(\n",
    "        predict_test_values1[\"label\"][:, i], test_data[\"label\"].data[:, i], **pt_style\n",
    "    )\n",
    "    axes[i].set(xlabel=f\"Predicted label {i}\", ylabel=f\"True label {i}\")\n",
    "    axes[i].axline([0, 0], slope=1, color=\"tab:green\", zorder=-100)\n",
    "_ = fig.suptitle(\"Test set: predicted vs. true labels\", fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model is doing a reasonable job of recovering the true labels, but the prediction error (variance) is large for the test set labels. We will now demonstrate how to improve this by adding a parameter to learn the systematic error.\n",
    "\n",
    "### Model 2: _Lux_ with an inferred vector of extra flux uncertainties\n",
    "\n",
    "We will now add a vector parameter to the model to learn the systematic error at each spectral pixel. We do this by specifying a custom {py:class}`~pollux.models.transforms.LinearTransform` instance with an additional parameter, `s`, to capture the systematic error. We will set the prior on this parameter to be a half-Normal distribution (a normal truncated at 0) with a mean of 0 and a standard deviation of 5 (i.e. we expect the systematic error to be small but allow the possibility of it being large). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = plx.LuxModel(latent_size=8)\n",
    "\n",
    "# Here we construct a LinearTransform instance with custom priors on the parameters.\n",
    "# The transform has only one parameter, `A`, which is a matrix of shape (n_flux,\n",
    "# n_latents). But we also include the special name `s` in the priors, which is a vector\n",
    "# of shape (n_flux,) that will be added in quadrature to the flux uncertainties when\n",
    "# evaluating the likelihood.\n",
    "flux_trans = LinearTransform(\n",
    "    output_size=n_flux,\n",
    "    param_priors={\n",
    "        \"A\": dist.Normal(0.0, 1.0),\n",
    "        \"s\": dist.HalfNormal(5.0).expand((n_flux,)),  # <--- the added parameter!\n",
    "    },\n",
    ")\n",
    "model2.register_output(\"flux\", transform=flux_trans)\n",
    "\n",
    "# We register the label output as before, but we could have also added an unknown\n",
    "# systematic uncertainty here\n",
    "model2.register_output(\"label\", transform=LinearTransform(output_size=n_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now optimize as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params2, svi_results2 = model2.optimize(\n",
    "    train_data,\n",
    "    rng_key=jax.random.PRNGKey(112358),\n",
    "    optimizer=numpyro.optim.Adam(1e-3),\n",
    "    num_steps=32768,\n",
    "    svi_run_kwargs={\"progress_bar\": False},\n",
    ")\n",
    "svi_results2.losses.block_until_ready()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then optimize and evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_params2 = {\n",
    "    \"label\": {\"A\": opt_params2[\"label\"][\"A\"]},\n",
    "    \"flux\": {\"A\": opt_params2[\"flux\"][\"A\"], \"s\": opt_params2[\"flux\"][\"s\"]},\n",
    "}\n",
    "\n",
    "test_opt_params2, test_svi_results2 = model2.optimize(\n",
    "    test_data,\n",
    "    rng_key=jax.random.PRNGKey(12345),\n",
    "    optimizer=numpyro.optim.Adam(1e-3),\n",
    "    num_steps=32_768,\n",
    "    fixed_params=fixed_params2,\n",
    "    svi_run_kwargs={\"progress_bar\": False},\n",
    ")\n",
    "test_svi_results2.losses.block_until_ready()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_test_values2 = model2.predict_outputs(\n",
    "    test_opt_params2[\"latents\"], fixed_params2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4), layout=\"constrained\")\n",
    "for i in range(predict_test_values2[\"label\"].shape[1]):\n",
    "    axes[i].plot(\n",
    "        predict_test_values2[\"label\"][:, i], test_data[\"label\"].data[:, i], **pt_style\n",
    "    )\n",
    "    axes[i].set(xlabel=f\"Predicted label {i}\", ylabel=f\"True label {i}\")\n",
    "    axes[i].axline([0, 0], slope=1, color=\"tab:green\", zorder=-100)\n",
    "_ = fig.suptitle(\"Test set: predicted vs. true labels\", fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see visually here that the model with the systematic error parameter is doing a better job of recovering the true labels, with much less scatter in the predictions. We can also compare the loss function values for the two models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svi_results1.losses[-1], test_svi_results2.losses[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model with the systematic error parameter (model 2) has a much lower loss value (which, here, is related to the negative log-posterior probability of the model).\n",
    "\n",
    "We can also compare the inferred systematic error parameter to the true systematic error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_s = all_data[\"flux\"].preprocessor.inverse_transform_err(\n",
    "    opt_params2[\"flux\"][\"s\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(inferred_s, label=\"Inferred systematic error\")\n",
    "plt.plot(true_systematic_err, label=\"True systematic error\")\n",
    "plt.xlabel(\"Spectral pixel\")\n",
    "plt.ylabel(\"Systematic error\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, we have demonstrated how to incorporate a systematic error term into the _Lux_ model to account for underestimated uncertainties in the data. We added a parameter to capture this for the spectral fluxes, per pixel. But we could have instead added a single value of the error inflation (i.e. for all pixels), or added a similar parameter for the label data. This can significantly improve the model's ability to accurately predict label values, as demonstrated on simulated data. \n",
    "\n",
    "More complex modifications of the models or additional parameters (e.g., adding a simultaneous model of the continuum flux shape) can also be incorporated, but that requires implementing a custom numpyro model. We will demonstrate this in a subsequent tutorial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
